\section{Coding}

\begin{enumerate}

\item {\em Why is everything in its own module?}

   If a subroutine is in a Fortran module, then at compile time, 
   there is argument checking.  This ensures that the right number
   and data types of arguments are present.  It makes the code safer.



\item {\em How do tags work when editing source?}

   Tags allow the editor to follow function/subroutine names and
   automatically switch you to the source code corresponding to that
   function.  Using tags in \maestro\ depends on the editor:

   \begin{itemize}

     \item {\tt vi}:

     In the build directory, type `{\tt make tags}'.  Then in {\tt
       vi}, move the cursor over a function name and use {\tt
       \verb|^|-]} to bring up the source corresponding to that
       function.  Use {\tt \verb|^|-t} to go back.  (Here, {\tt \verb|^|-}
       refers to the Control key.)

     \item {\tt emacs}:

     In the build directory, type `{\tt make TAGS}'.  Then in {\tt
       emacs}, move the cursor over a function name and use {\tt M-.}\
       to bring up the source corresponding to that function.  Use
       {\tt M-*} to go back.  (Here, {\tt M-} refers to the META key.)

   \end{itemize}

\end{enumerate}


\section{Compiling}

\begin{enumerate}

\item {\em What version of the Fortran standard is needed?}

  Some features of Fortran 2003 are used, in particular, the {\tt
    ISO\_C\_BINDING} feature of Fortran 2003 is needed to define a long
  integer type for some MPI operations in Fortran.  This is supported
  by most Fortran compilers even if they don't support the entire
  Fortran 2003 standard.

  We also rely on the Fortran 95 standard that specifies that any
  local allocated arrays are automatically deallocated when a
  subroutine ends.  Fortran 90 does not do this.  Most
  \maestro\ routines rely on this Fortran 95 feature.


\item {\em The code doesn't compile, but complains right away that there
   is ``{\tt No rule to make target `fabio\_c.c', needed by `t/Linux.gfortran.mpi/c.depends'}''}

   The environment variable {\tt AMREX\_HOME} needs to be the full path
   to the {\tt amrex/} directory.  You cannot use `{\tt $\sim$}' as a shortcut
   for your home directory.
  
\item {\em {\tt make} issues an error like:
       {\small
       \begin{verbatim}
       .../BoxLib/Tools/F_mk/GMakeMPI.mak:40: Extraneous text after `else' directive
       .../BoxLib/Tools/F_mk/GMakeMPI.mak:47: *** only one `else' per conditional.  Stop
      \end{verbatim}
      }
      }

   You need to use GNU make version 3.81 or later.  Earlier versions do
   not support an {\em else-if} clause.

\end{enumerate}

\section{Running}

\begin{enumerate}

\item {\em  How do we turn off all the initial projections to look at the
   initial velocity field as specified in initdata, instead of as
   modified by the velocity constraint?} 
%
\begin{verbatim}
    max_step  = 1
    init_iter = 0
    init_divu_iter = 0
    do_initial_projection = F
\end{verbatim}
\runparamidx{max\_step}
\runparamidx{init\_iter}
\runparamidx{init\_divu\_iter}
\runparamidx{do\_initial\_projection}

\item {\em  \maestro\ crashes because the multigrid algorithm fails to
    converge---how do I get around this?}

  Setting general convergence criteria for multigrid is as much
  art as science. 
  First, it is important to determine if the multigrd solver is
  close to convergence and just dancing around near the desired
  tolerance, but never reaching it, or if it is no where near 
  convergence.  For the latter, it may be that the multigrid
  solver was fed bad data and the problem arose in one of the earlier
  steps.  To get more detail information from the multigrid solver,
  set \runparam{mg\_verbose} to a positive integer from 1-4 (the higher
  the number the more information you receive.

  If the multigrid solver is failing during one of the initial
  ``divu'' iterations, it may be because the velocity is initially
  zero, so there is no velocity magnitude to use as a reference for
  convergence, and that ($S - \bar{S}$) is very small (or zero).  In
  this case, it is usually a good idea to perturb the initial state
  slightly, so the righthand side is non-zero.

  The tolerances used for the various multigrid solves in the code
  can be overridden on a problem-by-problem basis by putting a
  copy of {\tt MAESTRO/Source/mg\_eps.f90} into the problem directory
  and resetting the tolerances.  The role of each of these tolerances
  is described in {\tt MAESTRO/docs/mg/}.

\item {\em Why do the initial projection and ``divu'' iters sometimes
  have a harder time converging than the multigrid solves in the main algorithm?}

  The initial projection and ``divu'' solve sets the density to $1$
  (see \S~\ref{sec:flow:initialization}), so the coefficients in the
  elliptic solve are $O(\beta_0) \sim O(\rho)$.  But in the main
  algorithm, the coefficients are $O(\beta_0/\rho) \sim O(1)$.  Since
  $\rho$ can vary a lot, the variation in the coefficients in the
  initial projection and ``divu'' solve present a harded linear system
  to solve.


\item {\em How can I obtain profiling infomation for my run?}

  The code is already instrumented with timers.  Simply compile with
  {\tt PROF=TRUE} in the {\tt GNUmakefile}, or equvalently do 
  {\tt make PROF=t}.  A file containing a summary of the timings will
  be output in the run directory.

  An alternate way to get single-processor timings, when using the GCC
  compilers is to add {\tt -pg} to the compilation flags for both {\tt
    gfortran} and {\tt gcc}.  This can be accomplished by setting:
  \begin{verbatim}
    GPROF := t
\end{verbatim}
  in your {\tt GNUmakefile}.  Upon completion, a file
  names {\tt gmon.out} will be produced.  This can be processed by
  {\tt gprof} running

  \begin{verbatim}
    gprof exec-name
\end{verbatim}
  where {\em exec-name} is the name of the executable.  More detailed
  line-by-line timings can be obtained by using the {\tt -l} argument
  to {\tt gprof}.

\item{\em How can I force \maestro\ to abort?}

  In the output directory, do `{\tt touch .abort\_maestro}'.  This 
  will cause the code to write out a final checkpoint file, free up
  any allocated memory, and gracefully exit.  Be sure to remove the 
  {\tt .abort\_maestro} file before restarting the code in the 
  same directory.



\end{enumerate}


\section{Debugging}

\begin{enumerate}

\item {\em How can we dump out a variable to a plotfile from any point in the
   code?} 
%
\begin{verbatim}
    use fabio_module

    call fabio_ml_multifab_write_d(uold,mla%mba%rr(:,1),"a_uold")
    call fabio_ml_multifab_write_d(umac(:,1),mla%mba%rr(:,1),"a_umacx")
\end{verbatim}

\item {\em How can I print out a multifab's contents from within the code?}

  There is a {\tt print} method in {\tt multifab\_module}.  This can
  be simply called as
  \begin{verbatim}
  call print(a)
  \end{verbatim}
  where {\tt a} is a multifab (single-level).

\item {\em How can I debug a parallel (MPI) job with gdb?}

If you only need to use a few processors, the following command will work:
\begin{verbatim}
mpiexec -n 4 xterm -e gdb ./main.Linux.gfortran.mpi.exe 
\end{verbatim}
where the executable needs to be created with the ``{\tt -g}'' flag to
the compiler.  This will pop up multiple {\tt xterm}s with {\tt gdb} running
in each.  You need to then issue:
\begin{verbatim}
run inputs
\end{verbatim}
where {\tt inputs} is the desired inputs file {\em in each} {\tt xterm}.



\item {\em How can I get more information about floating point exceptions?}

\amrex\ can intercept floating point exceptions and provide a helpful
backtrace that shows you where they were generated.  See \S
\ref{ch:makefiles:special}.


\end{enumerate}


\section{I/O}

\begin{enumerate}


\item {\em How can I tell from a plotfile what runtime parameters were
   used for its run? or when it was created?}

   In each plotfile directory, there is a file called {\tt job\_info}
   (e.g.\ {\tt plt00000/job\_info}) that lists the build directory and
   date, as well as the value of every runtime parameter for the run.


\item {\em How can I force the code to output a plotfile / checkpoint
  file at the next step?}

   In the output directory (where the code is running) do `{\tt touch
     .dump\_plotfile}'.  This will create an empty file called {\tt
     .dump\_plotfile}.  At the end of each step, if the code finds
   that file, it will output a plotfile.  Simply delete the file to
   restore the code to its normal plotfile behavior.

   Similarly, creating the file {\tt .dump\_checkpoint} will force the
   output of a checkpoint file.  

\end{enumerate}



\section{Algorithm}

\begin{enumerate}

\item {\em Why is \maestro\ so ``hard'' to use (e.g.\ as compared to a
  compressible code)?}

There are several complexities to the algorithm that don't have
straightforward compressible counterparts.  These mainly involve the
role of the base state and the constraint equation.

Care must be taken to setup an initial model/initial base state that
respects the thermodynamics in \maestro\ and is in hydrostatic equilibrium.
Best results are attained when the model is processed with the \maestro\
EOS and reset into HSE, as is done in the {\tt initial\_model} routines.
Because \maestro\ builds off of the base state, any flaws in that initial 
state will influence the subsequent behavior of the algorithm.

The constraint equation brings another complexity not seen in compressible
codes---information is instantly communicated
across the grid.  In compressible codes you can track down a problem by
watching where it starts from and watching it move one cell per dt.  In
\maestro\ things can go wrong in multiple places without it being obvious
where the root problem is.




\item {\em In the final projection in the algorithm, we project
  $U^{n+1}$, using a time-centered $\beta_0$, a time-centered $\rho_0$, but
  an ``$n+1$''-centered $S$.  Why then is the resulting $\phi$ (which then
  defines $\pi$) is at ``$n+1/2$''?}

  The short answer to this question is that you should think of this
  as really projecting  $(U^{n+1} - U^n)$ and the right hand side as having
  $(S^{n+1} - S^n)$.  This is because the pressure enters the dynamic equations as
  $(U^{n+1} - U^n) = \ldots + \frac{1}{\rho^{n+1/2}} \nabla \pi^{n+1/2}$.
  (We approximate $\pi^{n+1/2}$ by $\pi^{n-1/2}$ then do the projection to fix the
  $\pi$ as well as the $U$.)

  So everything is in fact time-centered.


\item {\em Why is $\gammabar$ computed as the average of the full state
       $\Gamma_1$ instead of computed from the base state density and 
       pressure via the equation of state?}

 The primary reason is that there is no base state composition.  The
 base state density is simply the average of the full state density,
 and the base state pressure is the pressure required for hydrostatic
 equilibrium.  There is no thermodynamic relationship enforced between
 these base state quantities.

\item {\em Can I run a full star in 2-d axisymmetric geometry?}

 No.  This is a design decision.  There is no support for axisymmetric
 coordinates in \maestro.  Spherical problems must be run in 3-d.


\item {\em Why did we switch all the equations over to the
  $\tilde{\Ub}$ form instead of just working with $\Ub$?}

This is basically a numerical discretization issue.  Whenever the base
state aligns with the grid, you should be able to show that you get
exactly the same answer each way.

When you do a spherical star on a 3d Cartesian grid, though, the $w_0$
is defined on the radial mesh and the $\tilde{\Ub}$ on the Cartesian
mesh, and the $w_0$ part never experiences the Cartesian projection,
for example.  So there are differences in exactly how the $w_0$ component
appears (projected on the Cartesian mesh vs.\ interpolated from the
radial mesh)---we made the decision at the time to separate the
components for that reason.


\item {\em Why does ``checkerboarding'' appear in the velocity field,
especially in regions where the flow is stagnant?}

Checkerboarding can arise from the projection---it doesn't see that
mode (because it is an approximate projection) so it is unable to
remove it.  This allows the pattern to slowly build up.  There are
filtering techniques that can be used to remove these modes, but
they are not implemented in \maestro.

\end{enumerate}


\section{Analysis}

\begin{enumerate}

\item {\em I want to open a plotfile, derive a new quantity from
 the data stored there, and write out a new plotfile with this derived 
 data.  How do I do this?}

 One implementation of this can be found in {\tt
   amrex/Tools/Postprocessing/F\_Src/tutorial/fwrite2d.f90}.  This reads in
 the plotfile data using the {\tt plotfile\_module} that the {\tt
   data\_processing} routines rely on, but then builds a multifab
   and writes the data out to a plotfile using the \amrex\ write
   routines.

\end{enumerate}
