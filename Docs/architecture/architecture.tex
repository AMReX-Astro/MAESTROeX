\section{\maestro\ Basics}

\maestro\ models problems that are in tight hydrostatic equilibrium.
The fluid state is decomposed into a 1D radial base state that
describes the hydrostatic structure of the star or atmosphere, and a
2- or 3D Cartesian full state, that captures the departures from
hydrostatic equilibrium.  Two basic geometries are allowed.  A {\em
  plane-parallel} geometry assumes that the domain is thin compared to
the radius of curvature of the star, and therefore the 1D base state
is perfectly aligned with the Cartesian state.  A {\em spherical}
geometry is for modeling an entire star\footnote{Spherical geometry
  only exists for 3-d. This is a design decision---convection is 3-d.
  You can however run as an octant}.  Here, the 1D base state is not
aligned with the Cartesian state.  Figure~\ref{fig:base_state} shows
these geometries.

\begin{figure}[tb]
\centering
\includegraphics[height=2.0in]{\archfigpath/base_grid} \hspace{0.5in}
\includegraphics[height=2.0in]{\archfigpath/base_spherical}
\caption[\maestro\ geometries]{\label{fig:base_state} \maestro\ geometries, showing both the
  1D base state and the full Cartesian state.  (Left) For multi-level
  problems in planar geometry, we force a direct alignment between the
  radial array cell centers and the Cartesian grid cell centers by
  allowing the radial base state spacing to change with space and
  time.  (Right) For multi-level problems in spherical geometry, since
  there is no direct alignment between the radial array cell centers
  and the Cartesian grid cell centers, we choose to fix the radial
  base state spacing across levels. Figure taken
  from~\cite{multilevel}.}
\end{figure}


\maestro\ can use adaptive mesh refinement to focus resolution on
complex regions of flow.  For Cartesian/plane-parallel geometries, all
cells at the same height must be at the same level of refinement.
This restriction is to allow for the base state to directly align with
the Cartesian state everywhere.  For spherical geometries, there is no
such restriction (again, see Figure~\ref{fig:base_state}).
The \maestro\ grids are managed by the \amrex\ library, which is
distributed separately.



\section{The \maestro\ `Ecosystem'}

\begin{figure}[]
\centering
\includegraphics[scale=0.55]{\archfigpath/maestro_ecosystem2}
\caption[\maestro\ `ecosystem'] {\label{fig:arch:eco} The basic
  \maestro\ `ecosystem'.  Here we see the different packages that
  contribute to building the {\tt reacting\_bubble} problem in \maestro.  The
  red directories are part of most standard \maestro\ build.  The
  purple lines show the directories that are pulled in through
  various Makefile variables ({\tt AMREX\_HOME}, {\tt NETWORK\_DIR},
  {\tt EOS\_DIR}, and {\tt CONDUCTIVITY\_DIR}).}
\end{figure}


Building \maestro\ requires both the \maestro-specific source
files (distributed in the {\tt MAESTRO/} directory), and the
\amrex\ library (distributed separately, consisting of the {\tt amrex/} directory).
\amrex\ provides both a C++ and a Fortran framework.  \maestro\
only uses the Fortran portions of \amrex.  Figure~\ref{fig:arch:eco}
shows the relationship between the different packages, highlighting
what goes into defining a specific \maestro\ problem.

Problems piece together various \maestro\ directories, choosing a
reaction network, equation of state, and conductivity routine to build
an executable.  Briefly, the \maestro\ sub-directories are:

\begin{itemize}

\item {\tt MAESTRO/}

  The main \maestro\ algorithm directory.  

  Important directories under {\tt MAESTRO/} include:

  \begin{itemize}

  \item {\tt Docs/}

    Documentation describing the basic algorithm (including this
    document).

  \item {\tt Exec/}

    The various problem-setups.  Each problem in \maestro\ gets it own
    sub-directory under {\tt SCIENCE/}, {\tt TEST\_PROBLEMS/}, or {\tt
      UNIT\_TESTS/}.  The {\tt GNUmakefile} in the problem directory
    includes the instructions on how to build the executable,
    including what modules in {\tt Microphysics/} are used.  Any file that
    you place in a sub-directory here takes precedence over a file of
    the same name in {\tt MAESTRO/}.  This allows problems to have
    custom versions of the main \maestro\ routines (e.g.\ initial
    conditions via \code{initdata.f90}.  See \S~\ref{sec:makefile} and
    Chapter~\ref{ch:make} for details on the build system).

 
    \begin{itemize}
    \item {\tt SCIENCE/}

      \maestro\ problem directories for science studies.  These are
      the setups that have been used for science papers in the past,
      or are the basis for current science studies.

    \item {\tt TEST\_PROBLEMS/}
 
      \maestro\ problem directories for simple test problems that have
      been used while developing \maestro.  Many of these problems
      have appeared in the various papers describing the
      \maestro\ algorithm.

    \item {\tt UNIT\_TESTS/}

      Special \maestro\ problem directories that test only a single
      component of the \maestro\ algorithm.  These often have their
      own main drivers (\code{varden.f90}) that setup and initialize
      some data structures and then call only a few of the
      \maestro\ routines.  See Chapter~\ref{chapter:unit_tests} for details.

    \end{itemize}

  \item {\tt Microphysics/}\footnote{Note: many more compatible routines are available in the separate \microphysics\ git repo}

    The basic microphysics routines used by \maestro.  These are organized
    into the following sub-directories.

    \begin{itemize}
    \item {\tt conductivity/}

      Various routines for computing the thermal conductivity used in
      the thermal diffusion part of the algorithm.

    \item {\tt EOS/}

      The {\tt gamma\_law\_general/}.

    \item {\tt networks/}

      The basic {\tt general\_null} network that defines arbitrary
      non-reacting species.

    \end{itemize}


  \item {\tt Source/}

    The main \maestro\ source.  Here you will find the driver routine,
    the advection routines, etc.  All \maestro\ problems will compile
    this source.


  \item {\tt Util/}

    Various helper routines exist in this directory.  Some of these
    are externally developed.

    \begin{itemize}

    \item {\tt BLAS/}

      Linear algebra routines.

    \item {\tt initial\_models/}

      Simple routines for generating toy initial models in hydrostatic equilibrium.

    \item {\tt model\_parser/}

      A simple Fortran module for reading in 1D initial model files.
      This is used by the initialization routines to get the initial
      model data.

    \item {\tt random/}

      A random number generator.

    \item {\tt VODE/}

      The {\tt VODE} \cite{vode} package for integrating ODEs.  At the
      moment, this is used for integrating various reaction networks.
 
   \end{itemize}

  \end{itemize}

\end{itemize}


\begin{figure}[t]
\centering
\includegraphics[scale=0.6]{\archfigpath/amrex_directory2}
\caption[\amrex\ directory structure] {\label{fig:arch:amrex} The
  basic \amrex\ directory structure.  The directories used by
  \maestro\ are indicated in red.}
\end{figure}

The \amrex\ directory structure is shown in
Figure~\ref{fig:arch:amrex}.  The subset of the directories that are
used by \maestro\ are:
\begin{itemize}
\item {Src/}

  The main \amrex\ source directory.  In \maestro, we only use the
  Fortran source files.  The core directories are:

  \begin{itemize}

  \item {\tt F\_BaseLib/} 

    The Fortran \amrex\ files.  This is a library for describing
    meshes consisting of a union of boxes.  The \amrex\ modules
    define the basic datatypes used in \maestro.  \amrex\ also
    provides the routines that handle the parallelization and I/O.

  \item {\tt LinearSolvers/}

    The \amrex\ linear solvers---these are used to solve elliptic
    problems in the \maestro\ algorithm.

    \begin{itemize}

    \item {\tt F\_MG}

      The Fortran multigrid solver, with support for both
      cell-centered and node-centered data.

    \end{itemize}

  \end{itemize}

\item {\tt Tools/}

  Various tools used for building \amrex\ applications.  Here we use:

  \begin{itemize}

  \item {F\_mk/}

    The generic Makefiles that store the compilation flags for various
    platforms.  Platform/compiler-specific options are stored in the
    {\tt comps/} sub-directory.

  \item {\tt F\_scripts/}

    Some simple scripts that are useful for building, running,
    maintaining \maestro.

  \end{itemize}

\end{itemize}


Finally the {\tt amrex/Tools/Postprocessing/F\_Src} package provides simple
Fortran-based analysis routines (e.g.\ extract a line from a
multidimensional dataset) that operate on \amrex\ datasets.  These are
described in \S~\ref{sec:analysis}.  Several sub-directories with
python-based routines are also here.  These are described both in
\S~\ref{sec:analysis} and \S~\ref{sec:vis:python}.





\section{Adding A New Problem}
\label{sec:adding_problems}

Different \maestro\ problems are defined in sub-directories under {\tt
  Exec/} in {\tt SCIENCE}, {\tt TEST\_PROBLEMS}, or {\tt UNIT\_TESTS}.
To add a problem, start by creating a new sub-directory---this is
where you will compile your problem and store all the problem-specific
files.

The minimum requirement to define a new problem would be a {\tt
  GNUmakefile} which describes how to build the application and an
input file which lists the runtime parameters.  The problem-specific
executable is built in the problem directory by typing {\tt make}.
Source files are found automatically by searching the directories
listed in the {\tt GNUmakefile}.  Customized versions of any source
files placed in the problem-directory override those with the same
name found elsewhere.  Any unique source files (and not simply a
custom version of a file found elsewhere) needs to be listed in a file
called {\tt GPackage.mak} in the problem-directory (and this needs to
be told to the build system---see below).

\subsection{The {\tt GNUmakefile}}

\label{sec:makefile}

A basic {\tt GNUmakefile} begins with:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
  NDEBUG := t
  MPI    :=
  OMP    :=
\end{lstlisting}
Here, {\tt NDEBUG} is true if we are building an optimized executable.
Otherwise, the debug version is built---this typically uses less
optimization and adds various runtime checks through compiler flags.
{\tt MPI} and {\tt OMP} are set to true if we want to use either MPI
or OpenMP for parallelization.  If {\tt MPI := t}, you will need to
have the MPI libraries installed, and their location may need to be 
specified in {\tt MAESTRO/mk/GMakeMPI.mak}.

The next line sets the compiler to be used for compilation:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
  COMP := gfortran
\end{lstlisting}
The \maestro\ build system knows what options to use for various
compiler families.  The {\tt COMP} flag specifies which compiler to
use.  Allowed values include {\tt Intel}, {\tt gfortran}, {\tt PGI},
{\tt PathScale}, and {\tt Cray}.  The specific details of these
choices are defined in the {\tt MAESTRO/mk/comps/} directory.

{\tt MKVERBOSE} set to true will echo the build commands to the
terminal as the are executed.
\begin{lstlisting}[language={[gnu]make},mathescape=false]
  MKVERBOSE := t
\end{lstlisting}

The next line defines where the top of the \maestro\ source tree is located.
\begin{lstlisting}[language={[gnu]make},mathescape=false]
  MAESTRO_TOP_DIR := ../../..
\end{lstlisting}

A \maestro\ application is built from several packages (the
multigrid solver, an EOS, a reaction network, etc.).  The core
\maestro\ packages are always included, so a problem only needs
to define the EOS, reaction network, and conductivities to
use, as well as any extra, problem-specific files.  
\begin{lstlisting}[language={[gnu]make},mathescape=false]
EOS_DIR := helmholtz   
CONDUCTIVITY_DIR := constant
NETWORK_DIR := ignition_simple

EXTRA_DIR := Util/random
\end{lstlisting}
Note that the microphysics packages are listed simply by the name of 
the directory containing the specific implementation (e.g. {\tt helmholtz}).
By default, the build system will look in {\tt Microphysics/EOS/} for
the EOS, {\tt Microphysics/conductivity/} for the conductivity routine,
and {\tt Microphysics/networks/} for the reaction network.  To
override this default search path, you can set {\tt EOS\_TOP\_DIR},
{\tt CONDUCTIVITY\_TOP\_DIR}, and {\tt NETWORK\_TOP\_DIR} respectively.

Generally, one does not need to include the problem directory itself
in {\tt EXTRA\_DIR}, unless there are unique source files found there,
described in a {\tt GPackage.mak} file.  These variables are
interpreted by the {\tt GMaestro.mak} file and used to build a master
list of packages called {\tt Fmdirs}.  The build system will attempt
to build all of the files listed in the various {\tt GPackage.mak}
files found in the {\tt Fmdirs} directories.  Furthermore, {\tt
  Fmdirs} will be will be added to the {\tt make} {\tt VPATH}, which
is the list of directories to search for source files.  The problem
directory will always be put first in the {\tt VPATH}, so any source
files placed there override those with the same name found elsewhere
in the source tree.  

Some packages (for instance, the {\tt helmholtz}
EOS) require Fortran include files.  The {\tt Fmincludes} variable
lists all those directories that contain include files that are
inserted into the Fortran source at compile time via the {\tt include}
statement.  Presently, the only instance of this is with the Helmholtz
general equation of state found in {\tt Microphysics/EOS/helmholtz/}.  This is
automatically handled by the {\tt GMaestro.mak} instructions.

Runtime parameters listed in the {\tt MAESTRO/\_parameters} file are
parsed at compile time and the file \code{probin.f90} is written and
compiled.  This is a Fortran module that holds the values of the
runtime parameters and makes them available to any routine.  By
default, the build system looks for a file called {\tt \_parameters}
in the problem directory and adds those parameters along with the
master list of \maestro\ parameters ({\tt MAESTRO/\_parameters}) to
the \code{probin\_module}.

The final line in the {\tt GNUmakefile} includes the rules to actually
build the executable.
\begin{lstlisting}[language={[gnu]make},mathescape=false]
  include $(MAESTRO_TOP_DIR)/GMaestro.mak
\end{lstlisting}
%$


\subsubsection{Handling Problem-Specific Source Files}

As mentioned above, any source files placed in the problem directory
override a files with the same name found elsewhere in the source
tree.  This allows you to create a problem-specific version of any
routine.  Source files that are unique to this problem (i.e.\ there is
no file with the same name elsewhere in the source tree) need to be
listed in a file {\tt GPackage.mak} in the problem directory, and
the problem-directory needs to be explicitly listed in the {\tt EXTRA\_DIR}
list in the {\tt GNUmakefile}.


\subsection{Defining Runtime Parameters}

\label{sec:def_runtime_param}

The runtime parameters for the core \maestro\ algorithm are listed in
{\tt MAESTRO/\_parameters}.  That file is parsed at compile-time by
the {\tt MAESTRO/write\_probin.py} script (along with any
problem-specific parameters).  The script outputs the \code{probin.f90}
source file.  Each line in the {\tt \_parameters} file has the form:
\vskip 3mm
{\em parameter} \hskip 10em  {\em data-type} \hskip 10em  {\em value} 
\vskip 3mm
\noindent where {\em parameter} is the name of the runtime parameter,
{\em data-type} is one of \{{\tt character}, {\tt real}, {\tt
  integer}, {\tt logical}\}, and the {\em value} specifies the default
value for the runtime parameter.  Comments are indicated by a `{\tt
  \#}' character and are used to produce documentation about the
available runtime parameters.  For the documentation, runtime parameters are grouped together
in the {\tt \_parameters} file into categories.  The category headings
are defined by comments in the {\tt \_parameters} file and any comments
following that heading are placed into that category.  The documentation
(Chapter~\ref{ch:parameters}) is produced by the script 
{\tt MAESTRO/docs/runtime\_parameters/rp.py}.

At runtime, the default values for the parameters can be overridden
either through the inputs file (by adding a line of the form: {\tt
  parameter = value}) or through a command-line argument (taking the
form: {\tt --parameter value}).  The \code{probin\_module} makes the
values of the runtime parameters available to the various functions
in the code (see \S~\ref{sec:probin}).

Problem-specific runtime parameters should be defined in the
problem-directory in a file called {\tt \_parameters}.  This file will
be automatically found at compile time.



\subsection{Preparing the Initial Model}

\label{sec:initial_models}

\maestro\ models subsonic flows that are in hydrostatic equilibrium.
The solution in \maestro\ is broken up into a 1D base state and the 2-
or 3D full state.  The job of the 1D base state in the algorithm is
to represent the hydrostatic structure.  The full, Cartesian state
carries the departures from hydrostatic equilibrium.  The underlying
formulation of the low Mach number equations assumes that the base
state is in hydrostatic equilibrium.  At the start of a simulation,
the initial model is read in and taken as the base state.  Therefore,
any initial model needs to already be in hydrostatic equilibrium.

The routines in {\tt Util/initial\_models/} prepare an initial model
for \maestro.  In general, there are two different proceduces that are
needed.  The first type modify an existing 1D initial model produced
somewhere else (e.g.\ a 1D stellar evolution code), and map it onto a
uniform grid, at the desired resolution, using the equation of state
in \maestro, and using \maestro's discretization of hydrostatic
equilibrium.  The second type generate the initial model internally,
by integrating the condition of hydrostatic equilibrium together with
a simplifying assumption on the energy (e.g. isothermal or
isentropic).  In both cases hydrostatic equilibrium is enforced as:
\begin{equation}
\frac{p_{i+1} - p_i}{\Delta r} = \frac{1}{2} (\rho_i + \rho_{i+1})
g_{i+1/2}
\end{equation}
Here, $g_{i+1/2}$ is the edge-centered gravitational acceleration.

The {\tt toy\_atm} example provides a simple approximation for a thin
(plane-parallel) convectively-unstable accreted layer on the surface
of a star.  This can be used as the starting point for a more complex
model.  

\maestro\ initial models are read in by the \code{Util/model\_parser}
routines.  This expects the initial model to contain a header giving
the number of variables and their names, followed by rows of data
giving the coordinate and data values at that coordinate.  The initial
model should contain the same species data (in the form of mass fractions) as
defined in the \code{network} module used by the \maestro\ problem.

Full details on which initial model routine matches each problem and
how the initial models are used to initialize the full state data can
be found in \S~\ref{sec:initial_models_main}.

\subsection{Customizing the Initialization}

The best way to customize the initialization (e.g.\ add perturbations)
is to copy from one of the existing problems.  The file \code{initveldata.f90} controls the velocity field initialization and \code{initscaldata.f90} controls the initialization of the scalars
($\rho$, $\rho X_k$, $\rho h$).  The {\tt reacting\_bubble} problem is a good
starting point for plane-parallel and {\tt wdconvect} is a good
starting point for full stars.



\section{\amrex\ Data Structures}

\maestro's gridding is handled by the \amrex\ library, which
contains the most fundamental objects used to construct parallel
block-structured AMR applications---different
regions of the domain can have different spatial resolutions.
At each level of refinement, the region covered by that level is divided
into grids, or boxes.  The entire computational domain is covered by
the coarsest (base) level of refinement, often called level $\ell=0$, either by one
grid or divided into many grids.
Higher levels of refinement have cells that are finer by a ``refinement ratio''
(typically 2).  The grids are properly nested in the sense that the union 
of grids at level $\ell+1$ is contained in the union of grids at level $\ell$.
Furthermore, the containment is strict in the sense that, except at physical 
boundaries, the level $\ell$ grids are large enough to guarantee that there is
a border at least $n_{\rm buffer}$ level $\ell$ cells wide surrounding each level
$\ell +1$ grid (grids at all levels are allowed to extend to the physical
boundaries so the proper nesting is not strict there).  
For parallel computations, the boxes are spread across processors, in
a fashion designed to put roughly equal amounts of work on each
processor (load balancing).
\MarginPar{ghost cells?}

\begin{figure}[t]
\centering
\includegraphics[width=6.5in]{\archfigpath/data_loc2}
\caption[Data-centerings on the grid]
  {\label{fig:dataloc} Some of the different data-centerings:
  (a) cell-centered, (b) nodal in the $x$-direction, and (c) nodal in
  both the $x$- and $y$-directions.  Note that for nodal data, the
  integer index corresponds to the lower boundary in that direction.
  In each of these centerings, the red point has the same indices:\ (1,2).
  Not shown is the case where data is nodal in the $y$-direction only.}
\end{figure}
On a grid, the data can be stored at cell-centers, on a face/edge, or
on the corners.  In \amrex, data that is on an edge is termed `nodal'
in that direction (see Figure~\ref{fig:dataloc}).  Data that is on the
corners is nodal in all spatial directions.  In \maestro, the state
data (density, enthalpy, velocity, $\ldots$) is generally
cell-centered.  Fluxes are nodal in the direction they represent.
A few quantities are nodal in all directions (e.g.\ $\phi$ used in
the final velocity projection).

To simplify the description of the underlying AMR grid, \amrex\
provides a number of Fortran types.  We briefly summarize the major
data types below.  A more extensive introduction to \amrex\ is 
provided by the \amrex\ User's Guide, distributed with the library.


\subsection{\boxtype}

A \boxtype\ is simply a rectangular domain in space.  Note that boxes
do not hold the state data themselves.  A \boxtype\ has a {\tt lo} 
and {\tt hi} index in each coordinate direction that gives the
location of the lower-left and upper-right corner with respect to
a global index space.  

\begin{figure}[t]
\centering
\includegraphics[width=4.0in]{\archfigpath/index_grid2}
\caption[Single-level grid structure]
{\label{fig:boxes} Three boxes that comprise a single level.  At this
  resolution, the domain is 20$\times$18 zones.  Note that the
  indexing in \amrex\ starts with $0$.}
\end{figure}


The computational domain is divided into boxes.  The collection of
boxes with the same resolution comprise a level.
Figure~\ref{fig:boxes} shows three boxes in the same level of
refinement.  The position of the boxes is with respect to the global
index space at that level.  For example, box 1 in the figure has {\tt
  lo} = (3,7) and {\tt hi} = (9,12).  Note that the global indexing
is 0-based.

The global index space covers the entire domain at a given resolution.
For a simulation setup with \runparam{n\_cellx} {\tt = 32} and \runparam{n\_celly} {\tt =
  32}, the coarsest level (level 1) has $32 \times 32$ zones, and the
global index space will run from $0, \ldots, 31$ in each coordinate
direction.  Level 2 will have a global index space running from $0,
\ldots, 63$ in each coordinate direction (corresponding to $64 \times
64$ zones if fully refined), and level 3 will have a global index
space running from $0, \ldots, 127$ in each coordinate direction
(corresponding to $128\times 128$ zones if fully refined).


\subsubsection{Common Operations on a \boxtype}

A \boxtype\ declared as:
\begin{verbatim}
  type(box) :: mybox
\end{verbatim}
%
The upper and lower bounds of the box (in terms of the global
index space) are found via:
\begin{itemize}

\item {\tt lo = lwb(mybox)} returns an array, {\tt lo(dm)}, with
     the box lower bounds

\item {\tt hi = upb(mybox)} returns an array, {\tt hi(dm)}, with
     the box upper bounds

\end{itemize}




\subsection{\boxarray\ and \mlboxarray}

A \boxarray\ is an array of boxes.  A \mlboxarray\ is a collection of
\boxarray s at different levels of refinement.

\subsection{\layout\ and \mllayout}

A \layout\ is basically a \boxarray\ that knows information about other
boxes, or box ``connectivity.''  It contains additional information
that is used in filling ghost cells from other fine grids or from
coarser grids.  This information is stored as long as the layout
exists so that we don't have to recompute intersections every time we
do some operation with two \multifab s that have that layout, for
example.

By separating the layout from the actual data, we can allocate and
destroy data that lives on the grid as needed.


\subsection{\fab}

A \fab\ is a ``Fortran Array Box''.  It contains the state data in a
multidimensional array and several \boxtype-types to describe where in
the global index-space it lives:
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  type fab
     ...
     type(box) :: bx
     type(box) :: pbx
     type(box) :: ibx
  end type fab
\end{lstlisting}
{\tt bx} represents the box in the global index-space over which the
\fab\ is defined, {\tt pbx} represents the ``physical'' box in the
sense that it includes {\tt bx} plus ghost cells, and {\tt ibx} is the
same as {\tt bx} unless the \fab\ is nodal.  As can be seen in
Figure~\ref{fig:dataloc}, for the same grid nodal data requires one
more array element than cell-centered data.  To address this {\tt ibx}
is made by growing {\tt bx} by one element along all nodal dimensions.

It's important to note that all state data is stored in a
four-dimensional array {\em regardless of the problem's
  dimensionality}.  The array is {\tt (nx,ny,nz,nc)} in size, where
{\tt nc} is the number of components, for instance representing
different fluid variables, and {\tt (nx,ny,nz)} are the number of
cells in each respective spatial dimension.  For 2D problems, {\tt
  nz=1}.

A \fab\ would represent the data for a single box in the domain.
In \maestro, we don't usually deal with \fab s alone, but rather
we deal with \multifab s, described next.

\subsection{\multifab}

A \multifab\ is a collection of \fab s at the same level of
refinement.  This is the primary data structure that \maestro\
routine operate on.  A multilevel simulation stores the 
data in an array of \multifab s, where the array index refers
to the refinement level.

All \fab s in a given \multifab\ have the same number of ghost cells,
but different \multifab s can have different numbers of ghost cells
(or no ghost cells).

\subsubsection{Working with \multifab s}

To build a \multifab, we need to provide a \layout, the number of
components to store in the \multifab\, and the number of ghostcells.  In
\maestro\, the hierarchy of grids will be described by a single
\mllayout.  A \multifab\ can be declared and built at any time in a
simulation using the \mllayout, thereby allocating space at every
grid location in the simulation.  The sequence to build a \multifab\
appears as
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  type(multifab) :: mfab(nlevs)
  ...
  do n = 1, nlevs
     call multifab_build(mfab(n), mla%la(n), nc, ng)
  enddo
\end{lstlisting}
Here, {\tt nc} is the number of components and {\tt ng} is the number
of ghostcells.  The \multifab\ is built one level at a time, using the
\layout\ for that level taken from the \mllayout, {\tt mla}.

A common operation on a \multifab\ is to initialize it to $0$
everywhere.  This can be done (level-by-level) as
\begin{lstlisting}[language={[95]fortran},mathescape=false]
call setval(mfab(n), ZERO, all=.true.)
\end{lstlisting}
where {\tt ZERO} is the constant 0.0 from \code{bl\_constants\_module}.

The procedure for accessing the data in each grid managed by the
\multifab\ is shown in \S~\ref{sec:example}.  Subroutines to add,
multiply, or divide two \multifab s exist, as do subroutines to copy
from one \multifab\ to another---see
\code{amrex/Src/F\_BaseLib/multifab.f90} for the full list of
routines that work with \multifab s.


When you are done working with a \multifab, its memory can be freed by
calling \code{multifab\_destroy} on the \multifab.




\subsection{\bctower}

A \bctower\ holds the information about what boundary conditions are
in effect for each variable in a
\maestro\ simulation.  These are interpretted by the ghost cell filling 
routines.  See \S~\ref{sec:arch:bcs} for more detail.

\section{\maestro\ Data Organization}

The state of the star in \maestro\ is described by both a
multidimensional state and the 1D base state.  The full
multidimensional state is stored in \multifab s while the base state
is simply stored in Fortran arrays.  Here we describe the
major \maestro\ data-structures.




\subsection{`{\tt s}' \multifab s (fluid state)}

The fluid state (density, enthalpy, species, temperature, and tracer)
are stored together in a cell-centered multi-component \multifab,
typically named {\tt sold}, {\tt s1}, {\tt s2}, or {\tt snew}
(depending on which time-level it represents).  The enthalpy is stored
as $(\rho h)$, and the species are stored as partial-densities $(\rho
X_k)$.  The tracer component is not used at present time, but can
describe an arbitrary advected quantity.

Individual state variables should be indexed using the integer keys
provided by the \code{variables} module (see \S
\ref{sec:variables_module}).  For example, the integer {\tt rho\_comp}
will always refer to the density component of the state.

Note: the pressure is not carried as part of the `{\tt s}' \multifab s.


\subsection{`{\tt u}' \multifab s (fluid velocity)}

The fluid velocity at time-levels $n$ and $n+1$ is stored in
a cell-centered multi-component \multifab, typically named
{\tt uold} or {\tt unew}.  Here the {\tt dm}
components correspond to each coordinate direction.

\subsection{{\tt umac} (the MAC velocity)}

In creating the advective fluxes, we need the time-centered velocity
through the faces of the zone---the $x$-velocity on the $x$-edges, the
$y$-velocity on the $y$-edges, etc.\ (see figure~\ref{fig:mac}).  This
type of velocity discretization is termed the MAC velocity (after the
``marker-and-cell'' method for free boundaries in incompressible
flows \cite{harlowwelch:1965}).



\begin{figure}[t]
\centering
\includegraphics[width=2.5in]{\archfigpath/mac2}
\hspace{0.1in}
\begin{minipage}[b]{3.8in}
\caption[The MAC grid]
{\label{fig:mac} The MAC grid for the velocity.  
Here the $x$-velocity is on the $x$-edges (shown as the 
blue points) and the $y$-velocity is on the $y$-edges
(shown as the red points).
}\ \\
\end{minipage}
\end{figure}

The MAC velocities are allocated at each level of refinement, {\tt n},
by making a \multifab\ array where each of the {\tt dm} components is
nodal in its respective direction:
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  type(multifab) :: umac(nlevel,dm)

  do n=1,nlevel
     do comp=1,dm
        call multifab_build_edge(umac(n,comp), mla%la(n),1,1,comp)
     enddo
  enddo
\end{lstlisting}



\subsection{Base State Arrays}

The base state is defined by $\rho_0$, $p_0$, and $w_0$.  There is no
base state composition.  Other arrays are defined as needed, such as
$h_0$, the base state enthalpy.

The base state arrays are 2-dimensional, with the first dimension
giving the level in the AMR hierarchy and the second the radial index
into the base state.  For spherical geometries, the base state only
exists at a single level, so the first index will always be 1.  The
radial index is 0-based, to be consistent with the indexing for the
Cartesian state data.  For example, the base state density would be
dimensioned: {\tt rho0(nlevs,0:nr\_fine-1)}.  Here, {\tt nlevs} is the
number of levels of refinement and {\tt nr\_fine} is the number of
cells in the radial direction at the finest level of refinement.

For multilevel, plane-parallel geometry, all grids at the same height
will have the same resolution so that the full state data is always
aligned with the base state (see Figure~\ref{fig:base_state}).  Base
state data on coarse grids that are covered by fine grids is not
guaranteed to be valid.

For spherical problems, the base state resolution, $\Delta r$, is
generally picked to be finer than the Cartesian grid resolution,
$\Delta x$, i.e.\ $\Delta r < \Delta x$.  The ratio is controlled
by the parameter \runparam{drdxfac}.

Note there are no ghost cells for the base state outside of the
physical domain.  For plane-parallel, multilevel simulations, there
are ghostcells at the jumps in refinement---these are filled by the
\code{fill\_code\_base} routine.  The convention when dealing with the
base state is that we only access it inside of the valid physical
domain.  Any multi-dimensional quantity that is derived using the base
state then has its ghost cells filled by the usually \multifab\ ghost
cell routines.


\section{\maestro\ Helper Modules}

A number of \maestro\ modules appear frequently throughout the source.
Below, we describe some of the more common functionality of the most
popular modules.

\subsection{\tt average\_module}

The \code{average\_module} module provides a routine {\tt average} that takes
a multilevel \multifab\ array and averages the full Cartesian data
onto the 1D base state.

\subsection{{\tt eos\_module}}

The \code{eos\_module} provides the interface to the equation of 
state to connect the state variables thermodynamically.  It 
gets the information about the fluid species from the \code{network}
module (for example, the atomic number, $Z$, and atomic weight, $A$,
of the nuclei).

Presently there is a single EOS that comes with \maestro, {tt gamma\_law\_general},
but many more are available through the external \microphysics\ repo\footnote{\microphysics\ is
available at \url{https://github.com/starkiller-astro/Microphysics}.  \maestro\ will
find it via the {\tt MICROPHYSICS\_HOME} environment variable}.  The \microphysics\
EOSs share the same interface and can be compiled into \maestro\ directly.  
Here are the more popular EOSs:
\begin{itemize}
\item {\tt helmholtz} represents a general stellar equation 
      of state, consisting of nuclei (as an ideal gas), radiation,
      and electrons (with arbitrary degeneracy and degree of relativity).
      This equation of state is that described in \cite{timmes_eos}.

      A runtime parameter, \runparam{use\_eos\_coulomb}, is defined in
      this EOS to enable/disable Coulomb corrections.

\item {\tt gamma\_law\_general} assumes an ideal gas with a mixed 
     composition and a constant ratio of specific heats, $\gamma$:
      \begin{equation}
      p = \rho e (\gamma - 1) = \frac{\rho k_B T}{\mu m_p} 
      \end{equation}
     where $k_B$ is Boltzmann's constant and $m_p$ is the mass of the
     proton.
     The mean molecular weight, $\mu$, is computed assuming 
     electrically neutral atoms:
     \begin{equation}
     \mu = \left ( \sum_k \frac{X_k}{A_k} \right )^{-1}
     \end{equation}
     An option in the source code itself exists for treating the
     species as fully-ionized, but there is no runtime-parameter to
     make this switch.

\item {\tt multigamma} is an ideal gas equation of state where each
  species can have a different value of $\gamma$.  This mainly affects
  how the internal energy is constructed as each species, represented
  with a mass fraction $X_k$ will have its contribution to the total
  specific internal energy take the form of $e = p/\rho/(\gamma_k -                                               
  1)$.  The main thermodynamic quantities take the form:
\begin{align}
p &= \frac{\rho k T}{m_u} \sum_k \frac{X_k}{A_k} \\
e &= \frac{k T}{m_u} \sum_k \frac{1}{\gamma_k - 1} \frac{X_k}{A_k} \\
h &= \frac{k T}{m_u} \sum_k \frac{\gamma_k}{\gamma_k - 1} \frac{X_k}{A_k}
\end{align}
We recognize that the usual astrophysical $\bar{A}^{-1} = \sum_k                                                  
X_k/A_k$, but now we have two other sums that involve different
$\gamma_k$ weightings.

The specific heats are constructed as usual,
\begin{align}
c_v &= \left . \frac{\partial e}{\partial T} \right |_\rho =
    \frac{k}{m_u} \sum_k \frac{1}{\gamma_k - 1} \frac{X_k}{A_k} \\
c_p &= \left . \frac{\partial h}{\partial T} \right |_p =
    \frac{k}{m_u} \sum_k \frac{\gamma_k}{\gamma_k - 1} \frac{X_k}{A_k}
\end{align}
and it can be seen that the specific gas constant, $R \equiv c_p - c_v$ is
independent of the $\gamma_i$, and is simply $R = k/m_u\bar{A}$ giving the
usual relation that $p = R\rho T$.  Furthermore, we can show
\begin{equation}
\Gamma_1 \equiv \left . \frac{\partial \log p}{\partial \log \rho} \right |_s =
   \left ( \sum_k \frac{\gamma_k}{\gamma_k - 1} \frac{X_k}{A_k} \right ) \bigg /
   \left ( \sum_k \frac{1}{\gamma_k - 1} \frac{X_k}{A_k} \right ) =
\frac{c_p}{c_v} \equiv \gamma_\mathrm{effective}
\end{equation}
and $p = \rho e (\gamma_\mathrm{effective} - 1)$.

This equation of state takes several runtime parameters that can set the
$\gamma_i$ for a specific species:
\begin{itemize}
\item \runparam{eos\_gamma\_default}: the default $\gamma$ to apply for
  all species
\item \runparam{species\_X\_name} and \runparam{species\_X\_gamma}: set the $\gamma_i$
  for the species whose name is given as {\tt species\_X\_name} to the
  value provided by {\tt species\_X\_gamma}.  Here, {\tt X} can be one
  of the letters: {\tt a}, {\tt b}, or {\tt c}, allowing us to specify
  custom $\gamma_i$ for up to three different species.
\end{itemize}

\end{itemize}

The thermodynamic quantities are stored in a Fortran type {\tt eos\_t},
which has fields for all the thermodynamic inputs and outputs.  The 
type definition is brought in through \code{eos\_type\_module}.
\footnote{ Note: an older interface to the EOS exists, but is
  deprecated.  In this mode, the {\tt eos\_old\_interface} module declares 
  the variables that need appear in the old-style \code{eos} call
  argument list.  \maestro\ routines use these module variables in the
  EOS call to avoid having to declare each quantity in each routine
  that calls the EOS.  Most code has been updated to use the new interface.}

The first argument to the {\tt eos} call is an integer key that
specifies which thermodynamic variables (in addition to the mass
fractions) are used as input.  EOS input options are listed 
in table~\ref{arch:table:eosinput}.

   \begin{table}[h]
   \caption{\label{arch:table:eosinput} EOS input flags}
   \begin{center}
   \begin{tabular}{lc}
   \hline
   key            & input quantities \\
   \hline
   {\tt eos\_input\_rt}       & $\rho$, $T$ \\
   {\tt eos\_input\_rh}       & $\rho$, $h$ \\
   {\tt eos\_input\_tp}       & $T$, $p$ \\
   {\tt eos\_input\_rp}       & $\rho$, $p$ \\
   {\tt eos\_input\_re}       & $\rho$, $e$ \\
   {\tt eos\_input\_ps}       & $p$, $s$ \\
   \hline
   \end{tabular}
   \end{center}
   \end{table}



\subsection{{\tt fill\_3d\_module}}

The \code{fill\_3d\_module} provides routines that map from the 1D
base state to the full Cartesian 2- or 3D state.  Variations in the
routines allow for cell-centered or edge-centered data on either the
base state or full Cartesian state.

\subsection{\tt fundamental\_constants\_module}

The \code{fundamental\_constants\_module} provides a simple list of
various fundamental constants (e.g.\ Newton's gravitational constant)
in CGS units.

\subsection{{\tt geometry}}

\subsection{{\tt network}}

The \code{network} module defines the number species advected by the
code ({\tt nspec}), their ordering, and gives their basic properties
(like atomic number, $Z$, and atomic mass, $A$).  All \maestro\ problems
require a {\tt network} module, even if there are no reactions
modeled.  Many different reaction modules (containing different sets
of isotopes) exist in {\tt Microphysics/networks}.  The particular network
used by a problem is defined in the problem's {\tt GNUmakefile}.

To find the location of a particular species (for instance, ``carbon-12'')
in the allowed range of {\tt 1:nspec}, you do the following query:
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  ic12 = network_species_index("carbon-12")
\end{lstlisting}
If the resulting index is {\tt -1}, then the requested species was not
found.

\subsection{{\tt probin\_module}}

\label{sec:probin}

\code{probin\_module} provides access to the runtime parameters.
The runtime parameters appear simply as module variables.  To get the 
value of a parameter, one simply needs to `{\tt use probin\_module}'.
The preferred method is to add the `{\tt only}' clause to the
{\tt use} statement and explicitly list only those parameters that
are used in the routine.  Defining new runtime parameters is
described in \S~\ref{sec:def_runtime_param}.

\subsection{{\tt variables}}

\label{sec:variables_module}

The \code{variables} module provides integer keys to index the state
multifabs and other arrays dealing with the scalar quantities.  The
most commonly used keys are are list in table~\ref{arch:table:variables}.

\begin{table}[h]
\caption{\label{arch:table:variables} Common {\tt variables} module keys}
\begin{center}
\begin{tabular}{ll}
\hline
{\tt rho\_comp}  & density \\
{\tt rhoh\_comp} & density $\times$ specific enthalpy, $(\rho h)$ \\
{\tt spec\_comp} & first species partial density, $(\rho X_1)$ \\
{\tt temp\_comp} & temperature \\
\hline
\end{tabular}
\end{center}
\end{table}

The species indices are contiguous in the state array, spanning {\tt
  spec\_comp:spec\_comp-1+nspec}.  To find a particular species, a
query can be made through the \code{network} module, such as:
\begin{verbatim}
  ic12 = network_species_index("carbon-12")
\end{verbatim}
and then the \fab\ can be indexed using {\tt spec\_comp-1+ic12} to
get ``carbon-12''.
The \code{variables} module also provides keys for the plotfile
variables and boundary condition types.

Other keys in the {\tt variables} modules are reserved for boundary
conditions ({\tt foextrap\_comp} and {\tt hoextap\_comp}), the
projection of the pressure ({\tt press\_comp}), or constructing
the plotfile.


\section{\amrex\ Helper Modules}

There are a large number of modules in {\tt amrex/} that provide
the core functionality for managing grids.  Here we describe
the most popular such modules.


\subsection{{\tt bl\_types}}

The main purpose of this module is to define the Fortran kind {\tt dp\_t}
which is used throughout the code to declare double precision variables.

\subsection{{\tt bl\_constants}}

This module provides descriptive names for a number of common double precision
numbers, e.g.\ {\tt ONE = 1.0\_dp\_t}.  This enhances the readability of
the code.

\subsection{{\tt parallel}}

All MPI calls are wrapped by functions in the \code{parallel} module.  For
serial jobs, the wrappers simply do the requested operation on processor.
By wrapping the calls, we can easily switch between serial and parallel
builds.

\section{\label{sec:example} Example: Accessing State and MAC Data}

In \maestro, the state data is stored in a cell-centered multifab array
(the array index refers to the AMR level) and the MAC velocities are
stored in a 2D nodal multifab array (with indices referring to the AMR
level and the velocity component).  Here we demonstrate a typical way
to extract the state and MAC velocity data.

All \maestro\ routines are contained in a module, to allow for compile-time
argument checking.
\begin{lstlisting}[language={[95]fortran},mathescape=false]
module example_module

contains
\end{lstlisting}

The main interface to our routine is called {\tt example}---this will
take the \multifab s containing the data and then pass them to the
work routines, {\tt example\_2d} or {\tt example\_3d}, depending on
the dimensionality.  
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  subroutine example(mla,s,umac,dx,dt)

    use bl_types
    use multifab_module
    use ml_layout_module
    use variables, only: rho_comp
\end{lstlisting}

\noindent Here, the {\tt bl\_types} and \code{multifab\_module}
modules bring in the basic \amrex\ data types. Specifically, here,
\code{bl\_types} defines {\tt dp\_t} which is the {\tt kind} used for
declaring double precision data, and \code{multifab\_module} defines
the \multifab\ data type.  The \code{ml\_layout\_module} defines the
datatype for a \mllayout---many routines will take an \mllayout to
allow us to fill ghostcells.  The \code{variables} module is a
\maestro\ module that provides integer keys for indexing the state
arrays.  In this case the integer {\tt rho\_comp} refers to the
location in the state array corresponding to density.

Next we declare the subroutine arguments:

\begin{lstlisting}[language={[95]fortran},mathescape=false]
    type(ml_layout), intent(in   ) :: mla
    type(multifab) , intent(inout) :: s(:)
    type(multifab) , intent(inout) :: umac(:,:)
    real(kind=dp_t), intent(in   ) :: dx(:,:),dt
\end{lstlisting}
Here, {\tt s(:)} is our \multifab\ array that holds the state data.
with the array index in {\tt s} refers to the AMR level.  The MAC 
velocities are held in the multifab {\tt umac}, with the array
indices referring to the AMR level and the component.


Local variable declarations come next:
\begin{lstlisting}[language={[95]fortran},mathescape=false]
    ! Local variables
    real(kind=dp_t), pointer :: sp(:,:,:,:)
    real(kind=dp_t), pointer :: ump(:,:,:,:), vmp(:,:,:,:), wmp(:,:,:,:)
    integer :: i,n,dm,nlevs,ng_sp,ng_um
    integer :: lo(mla%dim),hi(mla%dim)
\end{lstlisting}

\noindent Amongst the local variables we define here are a pointer,
{\tt sp}, that will point to a single \fab\ from the
\multifab\ {\tt s}, and a pointer for each component of the MAC
velocity, {\tt ump}, {\tt vmp}, and {\tt wmp} (for a 2D run,
we won't use {\tt wmp}).  We note that regardless of the dimensionality,
these pointers are 4-dimensional: 3 spatial + 1 component.

Next we get the dimensionality and number of levels
\begin{lstlisting}[language={[95]fortran},mathescape=false]
    dm = mla%dim
    nlevs = mla%nlevel
\end{lstlisting}


Each multifab can have their own number of ghostcells, so we get
these next:
\begin{lstlisting}[language={[95]fortran},mathescape=false]
    ng_sp = nghost(s(1))
    ng_um = nghost(umac(1,1))
\end{lstlisting}
By convention, all levels in a given multifab have the same number of
ghostcells, so we use level 1 in the \code{nghost()} call.  We also use
the same number of ghostcells for each component of the velocity, so
we only need to consider the first component in the {\tt nghost()}
call.  The ghostcells will be needed to access the data stored in the
\fab s.

To access the data, we loop over all the levels, and all the boxes in
the given level.
\begin{lstlisting}[language={[95]fortran},mathescape=false]
    do n=1,nlevs
       do i = 1, nfabs(s(n))
\end{lstlisting}
\code{nfabs(s(n))} is simply the number of boxes in level {\tt n} on
the current processor.  Each processor knows which \fab s in its
\multifab are local to that processor, and this loop will only loop
over those.

For a given \boxtype, we get the data and the bounds of the \boxtype.
\begin{lstlisting}[language={[95]fortran},mathescape=false]
          sp  => dataptr(s(n), i)
          ump => dataptr(umac(n,1),i)
          vmp => dataptr(umac(n,2),i)
          lo =  lwb(get_box(s(n), i))
          hi =  upb(get_box(s(n), i))
\end{lstlisting}


The actual data array is accessed through the {\tt dataptr} function,
which takes a \multifab\ (e.g.\ {\tt s(n)}) and the index of the
\boxtype\ ({\tt i}) we want.  We see that the $x$ MAC velocity for the
current box is stored in {\tt ump} and the $y$ MAC velocity is stored
in {\tt vmp}.  We don't get the $z$ velocity data here, since that
would not be available for a 2D run---we defer that until we test on
the dimensionality below.

Finally, the index bounds of the box (just the data, not the ghostcells) are 
stored in the {\tt dm}-dimensional arrays {\tt lo} and {\tt hi}.  These indices
refer to the current box, and hold for both the state, {\tt sp}, and the MAC
velocity, {\tt ump} and {\tt vmp}.  However, since the MAC velocity is nodal
in the component direction, the loops over the valid data will differ
slight (as we see below).

With the data extracted, we call a subroutine to operate on it.  We use
different subroutines for the different dimensionalities (and many times
have a separate routine for spherical geometries).
\begin{lstlisting}[language={[95]fortran},mathescape=false]
          select case (dm)
          case (2)
             call example_2d(sp(:,:,1,rho_comp),ng_sp, &
                             ump(:,:,1,1),vmp(:,:,1,1),ng_um, &
                             lo,hi,dx(n,:),dt)
          case (3)
             wmp => dataptr(umac(n,3),i)
             call example_3d(sp(:,:,:,rho_comp),ng_sp, &
                             ump(:,:,:,1),vmp(:,:,:,1),wmp(:,:,:,1),ng_um, &
                             lo,hi,dx(n,:),dt)
          end select
       enddo    ! end loop over boxes

    enddo    ! end loop over levels

  end subroutine example
\end{lstlisting}
\noindent We call either the function
{\tt example\_2d} for two-dimensional data or {\tt example\_3d}
for three-dimensional data.  Note that in the two-dimensional
case, we index the data as {\tt sp(:,:,1,rho\_comp)}.  Here a
`{\tt 1}' is used as the `z'-coordinate spatial index, since this
is a 2D problem, and the density component of the state is selected
(using the integer key {\tt rho\_comp}).  The 3D version accesses
the data as {\tt sp(:,:,:,rho\_comp)}---only the component regarding
the variable is needed here.  Notice that we also pass through
the number of ghostcells for each of the quantities.

This routine will be supplimented with {\tt example\_2d} and {\tt
example\_3d}, which actually operate on the data.  The form of 
the 2D function is:

\begin{lstlisting}[language={[95]fortran},mathescape=false]
  subroutine example_2d(density,ng_sp, &
                        umac,vmac,ng_um, &
                        lo,hi,dx,dt)

    use bl_constants_module
    use probin_module, only: prob_lo

    integer        , intent(in) :: lo(:),hi(:), ng_sp, ng_um
    real(kind=dp_t), intent(in) :: density(lo(1)-ng_sp:,lo(2)-ng_sp:)
    real(kind=dp_t), intent(in) ::    umac(lo(1)-ng_um:,lo(2)-ng_um:)
    real(kind=dp_t), intent(in) ::    vmac(lo(1)-ng_um:,lo(2)-ng_um:)

    real(kind=dp_t), intent(in) :: dx(:),dt

    integer         :: i, j
    real(kind=dp_t) :: x, y
    real(kind=dp_t) :: dens, u, v

    do j = lo(2), hi(2)
       y = prob_lo(2) + (dble(j) + HALF)*dx(2)

       do i = lo(1), hi(1)
          x = prob_lo(1) + (dble(i) + HALF)*dx(1)

          dens = density(i,j)

          ! compute cell-centered velocity
          u = HALF*(umac(i,j) + umac(i+1,j))
          v = HALF*(vmac(i,j) + vmac(i,j+1))

          ! operate on the data
          ! ...

       enddo
    enddo

  end subroutine example_2d

end module example_module
\end{lstlisting}

\noindent In this function, the bounds of the {\tt density} array take
into account the {\tt ng\_sp} ghostcells and the index space of the
current box.  Likewise, the MAC velocities refer to the {\tt ng\_um}
ghostcells.  The {\tt j} and {\tt i} loops loop over all the valid
zones.  Coordinate information is computed from {\tt dx} and {\tt
  prob\_lo} which is the physical lower bound of the domain.
\code{bl\_constants\_module} declares useful double-precision
constants, like {\tt HALF} (0.5).  Here, we see how to access the
density for the current zone and compute the cell-centered velocities
from the MAC velocities.  By convection, for a nodal array, the
indices refer to the {\em lower} interface in the nodal direction, so
for {\tt umac}, {\tt umac(i,j)} and {\tt umac(i+1,j)} are the $x$ MAC
velocities on the lower and upper edge of the zone in the
$x$-direction.

The three-dimensional case is similar, with the {\tt density} array
declared as 
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  density(lo(1)-ng_sp:,lo(2)-ng_sp:,lo(3)-ng_sp:)
\end{lstlisting}
and an additional loop over the `z' coordinate (from {\tt lo(3)} to
{\tt hi(3)}).

In this example, we looped over the valid zones.  If we wished to loop
over the interfaces bounding the valid zones, in the $x$-direction,
we would loop as
\begin{lstlisting}[language={[95]fortran},mathescape=false]
  do j = lo(2), hi(2)
     do i = lo(1), hi(1)+1
        ! access umac(i,j)
     enddo
  enddo
\end{lstlisting}


\section{Filling Ghostcells}

Ghostcells are filled through a variety of different routines, depending
on the objective.

\begin{itemize}

\item \code{multifab\_fill\_boundary} fills ghost cells for two
  adjacent grids at the same level, which als includes periodic domain
  boundary ghost cells.

\item \code{multifab\_physbc} fills ghostcells at the physical boundaries.

\item \code{multifab\_fill\_ghost\_cells} is used for multilevel
  problems, and fills ghostcells in the finer grid (level {\tt n}) by
  interpolating from data in the coarser grid (level {\tt n-1}).
  This function, by default, will also call {\tt multifab\_fill\_boundary}
  and {\tt multifab\_physbc} for both levels {\tt n} and {\tt n-1} (you 
  can override this behavior for speed optimization purposes).
  This call is usually preceded by a call to 
  \code{ml\_cc\_restriction\_c} which sets the level {\tt n-1} data to be
  the average of the level {\tt n} data covering it.
   
\end{itemize}

You generally won't see calls in the MAESTRO source code to these subroutines,
as there is now a special \amrex\ subroutine, {\tt ml\_restrict\_and\_fill},
that takes an array of multifabs at different level, and in order calls:
(1) {\tt ml\_cc\_restriction\_c}, (2) {\tt multifab\_fill\_boundary},
(3) {\tt multifab\_physbc}, and (4) {\tt multifab\_fill\_ghost\_cells}.
These four subroutines are called in such a way to avoid extra
ghostcell filling, saving on communication time.  You can specify the
starting component, starting boundary condition component, 
the number of components, the number of ghost cells,
and whether or not you want to use the same boundary condition component
for all variables.

\section{Boundary Conditions}

\label{sec:arch:bcs}

When \maestro\ is run, the boundary condition parameters are read in
from the input file and used to build the \bctower.  The
\bctower\ consists of a \bclevel\ object for each level of resolution
in the simulation.  The \bclevel\ contains 3 different descriptions of
the boundary conditions for each box in the domain at that level of
refinement: {\tt phys\_bc\_level\_array}, {\tt adv\_bc\_level\_array},
and {\tt ell\_bc\_level\_array}.  In all cases, the boundary
conditions are specified via integer values that are defined in {\tt
  bc\_module} (part of \amrex).

Each level has a {\tt phys\_bc\_level\_array(0:ngrids,dim,2)} array,
where {\tt ngrids} is the number of boxes on that level, {\tt dim} is
the coordinate direction, and the last index refers to the lower (1)
or upper (2) edge of the box in the given coordinate direction.  This
stores the {\em physical desciption} of the boundary type (outlet, inlet,
slipwall, etc.)---this description is independent of the variables
that live on the grid.  The {\tt phys\_bc\_level\_array(0,:,:)} `box'
refers to the entire domain.  If an edge of a box is not on a physical
boundary, then it is set to a default value (typically {\tt
  INTERIOR}).  These boundary condition types are used to interpret
the actual method to fill the ghostcells for each variable, as
described in {\tt adv\_bc\_level\_array} and {\tt
  ell\_bc\_level\_array}.


Whereas {\tt phys\_bc\_level\_array} provides a physical description
of the type of boundary, the array {\tt adv\_bc\_level\_array}
describes the {\em action} taken (e.g.\ reflect, extrapolate, etc.)
for each variable when filling boundaries.  {\tt
  adv\_bc\_level\_array} specifically describes the boundary
conditions that are in play for the advection (hyperbolic) equations.
The form of this array is {\tt
  adv\_bc\_level\_array(0:ngrids,dim,2,nvar)} where the additional
component, {\tt nvar}, allows for each state variable that lives on a
grid to have different boundary condition actions associated with it.
The convention is that the first {\tt dm} variables in \bclevel (where {\tt dm} is
the dimensionality of the simulation) refer to the
velocity components, and the subsequent slots are for the other
standard variables described in the \code{variables\_module}.  For
instance, to reference the boundary condition for density, one would
index with {\tt dm+rho\_comp}.  For temporary variables that are
created on the fly in the various routines in \maestro\ there may not
be a variable name in {\tt variables\_module} that describes the
temporary variable.  In this case, the special variables {\tt
  foextrap\_comp} and {\tt hoextrap\_comp} (first-order and high-order
extrapolation) are used.

{\tt ell\_bc\_level\_array} is the analog to {\tt
  adv\_bc\_level\_array} for the elliptic solves in \maestro.  This
will come into play in the multigrid portions of the code.  The
actions that are used for {\tt ell\_bc\_level\_array} are either
Dirichlet or Neumann boundary condtions.  For the velocity
projections, we are dealing with a pressure-like quantity, $\phi$, so
the pressure boundary conditions here reflect the behavior we want for
the velocity.  After the projection, it is $\nabla \phi$ that modifies
the velocity field.  At a wall or for inflow conditions, we already
have the velocity we want at the boundary, so we want the velocity to
remain unchanged after the projection.  This requires $d\phi/dn=0$ on
those boundaries.  For outflow, we impose a condition that we do not
want the boundaries to introduce any tangental acceleration (or
shear), this is equivalent to setting $\phi = 0$ (then $\partial
\phi/\partial t = 0$, with $t$ meaning `tangental').  This allows the
velocity to adjust as needed to the domain (see, for example,
\cite{almgrenBellSzymczak:1996}).

The actual filling of the ghostcells according to the descriptions
contained in the \bctower\ is carried out by the \code{multifab\_physbc} routine.  When you have an {\tt EXT\_DIR}
condition in {\tt multifab\_physbc} (an specified in the inputs file
as {\tt inlet}), the advection solver (via the slope routine) and
linear solvers will then assume that the value in the ghost cells is
equal to the value that actually lies on the wall.






\section{Multigrid}

\maestro\ uses the multigrid solver to enforce the divergence
constraint both on the half-time edge-centered advective velocities
(the ``MAC projection'') and on the final cell-centered velocities
(the ``HG projection'').  For the MAC projection, since the velocity
data is edge-centered (the MAC grid), the projection is cell-centered.
For the HG projection, since the velocity data is cell-centered, the
projection is node-centered.  \MarginPar{should draw a figure} The
multigrid solver performs a number of V-cycles until the residual
drops by 10-12 orders-of-magnitude.  There are several options that
affect how the multigrid solver behaves, which we describe below.
More detail on the multigrid solvers is given in Chapter~\ref{ch:mg}.


\section{Multilevel and Refinement Criteria}


\section{Particles}

\label{arch:sec:particles}

\maestro\ has support for Lagrangian particles that are passively
advected with the velocity field.  These are useful for diagnostics
and post-processing.  To use particles, particles must be seeded into
the domain by writing a problem-specific \code{init\_particles.f90}
routine.  This routine is called at the start of the simulation.  The
{\tt init\_particles} routines add particles at specific locations by
calling the \code{particle\_module}'s {\tt add} routine when a given
criteria is met by the fluid state.

When you run the code, particles are enabled by setting {\tt
  use\_particles = T}.  At the end of each timestep the locations of
all the particles are written out into a series of files called {\tt
  timestamp\_NN}, where {\tt NN} is the CPU number on which the
particle {\em currently} resides.  Particles are always kept on the
processor containing the state data corresponding to their present
location.  Several bits of associated data (density, temperature, and
mass fractions) are stored along with the particle ID and position.

Some simple python scripts allow for the plotting of the particle
positions.  See \S~\ref{analysis:sec:particles} for details.


\section{Regression Testing}

There is an extensive regression test suite for \amrex\ that works with
\maestro.  Full details, and a sample \maestro\ configuration file are
provided in the \amrex\ User's Guide and source.
